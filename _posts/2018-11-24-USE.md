---
layout: post
title: Universal Sentence Encoder
---

## What is the Universal Sentence Encoder (USE)?
Google's Universal Sentence Encoder (USE) is a tool that converts a string of words into 512 dimensional vectors. These vectors capture the semantic meaning of the sequence of words in a sentence and therefore can be used as inputs for other downstream NLP tasks like classification, semantic similarity measurement etc. A representative flow for a typical text classification task is shown below:

![_config.yml]({{ site.baseurl }}/images/example-classification (1).png)

USE is made available publicly on TensorFlow Hub. Following is a sample code that shows how to use it:

```python
import tensorflow_hub as hub

embed = hub.Module("https://tfhub.dev/google/"
"universal-sentence-encoder/1")
embedding = embed([
"The quick brown fox jumps over the lazy dog."])
```


## But wait...don't get too excited just yet!

While USE is a fantastic tool and one maybe tempted to use it straight out of the box, it is critical to know the underlying architecture of the model and most importantly, what kind of data it was trained on. For example if you're working with a corpus of scientific articles, it is useful to know that USE is trained on Wikipedia, which has plenty of scientific content. Thus, making the encoder an ideal choice for working with this kind if data. On the other hand, if you're dealing with bank reserach reports or other kinds of business documents, USE might encounter a lot of jargon that it is unfamiliar with which may lead to erroneous results. We shall cover this later in this post.

The Universal Sentence Encoder comes in 2 flavors each with a distinct model architecture:
### 1. The Transformer Architecture 

The Transformer based Universal Sentence Encoder constructs sentence embeddings using the encoder part of the transformer architecture proposed in this [paper](https://arxiv.org/pdf/1706.03762.pdf) by Vaswani et al. On a high level, this architecture uses self-attention to compute context aware representations of words in a sentence that take into account both the **ordering** and **identity** of all the other words. These context aware word vectors are them summed up to output the vector for the entire sentence. A pictorial representation of one encoder is shown below. There are 6 such encoders stacked one on top of the other in the final model. 

![_config.yml]({{ site.baseurl }}/images/Transformer.png)

It is important to note that even though the 'sequentiality' (for lack of better term) of the words is taken into account in this model, we do not use any form of recurrence here. This makes this model highly parallizable. If you would like to delve deeper into this cool architecture and understand its nuts and bolts under the hood, I'd recommend [this awesome blog on the transformer architecture.](http://jalammar.github.io/illustrated-transformer/) 

### 2. The Deep Averaging Network (DAN)

In this architecture the word embeddings for a sentence are first averaged together and then passed through a feed forward neural network as shown below.

![_config.yml]({{ site.baseurl }}/images/DAN.png)

The feed forward neural network is then trained on various downstream tasks like classification, similarity etc. The main advantage of the DAN over the transformer architecture is that the compute time is linear in length of the input sequence. 

The easiest way to make your first post is to edit this one. Go into /_posts/ and update the Hello World markdown file. For more instructions head over to the [Jekyll Now repository](https://github.com/barryclark/jekyll-now) on GitHub.


## The Data on which USE is trained

As I had touched upon earlier, it is important to know what kind of data the encoder was trained on. This will give you an idea of what tasks USE is suitable for. I have tabulated the data and task type here:


 **Task**  |  **Data**  
------------ | -------------
 Unsupervised  |  Wikipedia, Web news, Web question-answer pages and discussion forums.  
Supervised  |  Stanford Natural Language Inference (SNLI) corpus.  
{: .tablelines}


## Classification Example - IMDB movie review sentiment classification

We will now look at USE in action. [Movie reviews from IMDB] (http://ai.stanford.edu/~amaas/data/sentiment/) will be fed into the encoder which will convert them to fixed length 512 dimensional vectors that will then go into a Fully connected Neural Netwrok. Let's dive right into it!

### 1. Setting up the environment:

#### 1.1 Installing required packages
```python
# Install the latest Tensorflow version.
!pip install --quiet "tensorflow>=1.7"
# Install TF-Hub.
!pip install tensorflow-hub
!pip install seaborn
```
#### 1.2 Importing relevant libraries

```python
import tensorflow as tf
import tensorflow_hub as hub
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import re
import seaborn as sns
from tensorflow import keras
import tensorflow as tf
import tensorflow_hub as hub
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import re
import seaborn as sns

import keras.layers as layers
from keras.models import Model
from keras import backend as K
np.random.seed(10)
```

### 2. Downloading data and storing it as a DataFrame

```python
# Load all files from a directory in a DataFrame.
def load_directory_data(directory):
  data = {}
  data["sentence"] = []
  data["sentiment"] = []
  for file_path in os.listdir(directory):
    with tf.gfile.GFile(os.path.join(directory, file_path), "r") as f:
      data["sentence"].append(f.read())
      data["sentiment"].append(re.match("\d+_(\d+)\.txt", file_path).group(1))
  return pd.DataFrame.from_dict(data)

# Merge positive and negative examples, add a polarity column and shuffle.
def load_dataset(directory):
  pos_df = load_directory_data(os.path.join(directory, "pos"))
  neg_df = load_directory_data(os.path.join(directory, "neg"))
  pos_df["polarity"] = 1
  neg_df["polarity"] = 0
  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)

# Download and process the dataset files.
def download_and_load_datasets(force_download=False):
  dataset = tf.keras.utils.get_file(
      fname="aclImdb.tar.gz", 
      origin="http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz", 
      extract=True)
  
  train_df = load_dataset(os.path.join(os.path.dirname(dataset), 
                                       "aclImdb", "train"))
  test_df = load_dataset(os.path.join(os.path.dirname(dataset), 
                                      "aclImdb", "test"))
  
  return train_df, test_df

# Reduce logging output.
tf.logging.set_verbosity(tf.logging.ERROR)

train_df, test_df = download_and_load_datasets()
train_df.head()
```

Here's what the training dataframe looks like:

```python
train_df.shape
```

![_config.yml]({{ site.baseurl }}/images/IMDB_train.png)


```python

module_url = "https://tfhub.dev/google/universal-sentence-encoder-large/3" #@param ["https://tfhub.dev/google/universal-sentence-encoder/2", "https://tfhub.dev/google/universal-sentence-encoder-large/3"]

embed = hub.Module(module_url)

```
Now that we have loaded the encoder module, 2 corresponds to the DAN architecture while -large/3 corresponds to the Transformer.

We setup the model as follows:

```python

from keras.optimizers import Adam

optimizer=Adam(lr=0.001, decay=1e-4)
embed_size = 512

input_text = layers.Input(shape=(1,), dtype=tf.string)
embedding = layers.Lambda(UniversalEmbedding, output_shape=(embed_size,))(input_text)
dense1 = layers.Dense(128, activation='relu')(embedding)
dropout1 = layers.Dropout(0.1)(dense1)
#dense2 = layers.Dense(128, activation='relu')(dropout1)
#dropout2 = layers.Dropout(0.1)(dense2)
pred = layers.Dense(2, activation='softmax')(dropout1)
model = Model(inputs=[input_text], outputs=pred)
model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
model.summary()

```























