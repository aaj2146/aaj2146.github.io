---
layout: post
title: Universal Sentence Encoder
---

## What is the Universal Sentence Encoder (USE)?
Google's Universal Sentence Encoder (USE) is a tool that converts a string of words into 512 dimensional vectors. These vectors capture the semantic meaning of the sequence of words in a sentence and therefore can be used as inputs for other downstream NLP tasks like classification, semantic similarity measurement etc. A representative flow for a typical text classification task is shown below:

![_config.yml]({{ site.baseurl }}/images/example-classification (1).png)

## But wait...don't get too excited just yet!

While USE is a fantastic tool and one maybe tempted to use it straight out of the box, it is critical to know the underlying architecture of the model and most importantly, what kind of data it was trained on. For example if you're working with a corpus of scientific articles, it is useful to know that USE is trained on Wikipedia, which has plenty of scientific content. Thus, making the encoder an ideal choice for working with this kind if data. On the other hand, if you're dealing with bank reserach reports or other kinds of business documents, USE might encounter a lot of jargon that it is unfamiliar with which may lead to erroneous results. 

The Universal Sentence Encoder comes in 2 flavors each with a distinct model architecture:
### 1. The Transformer Architecture 

The Transformer based Universal Sentence Encoder constructs sentence embeddings using the encoder part of the transformer architecture proposed in this [paper](https://arxiv.org/pdf/1706.03762.pdf) by Vaswani et al. On a high level, this architecture uses self-attention to compute context aware representations of words in a sentence that take into account both the **ordering** and **identity** of all the other words. These context aware word vectors are them summed up to output the vector for the entire sentence. A pictorial representation of one encoder is shown below. There are 6 such encoders stacked one on top of the other in the final model. 

![_config.yml]({{ site.baseurl }}/images/Transformer.png)

If you would like to delve deeper into this cool architecture and understand it under the hood, I'd recommend [this awesome blog on the transformer architecture](http://jalammar.github.io/illustrated-transformer/) 

### 2. The Deep Averaging Network (DAN)

![_config.yml]({{ site.baseurl }}/images/DAN.png)

The easiest way to make your first post is to edit this one. Go into /_posts/ and update the Hello World markdown file. For more instructions head over to the [Jekyll Now repository](https://github.com/barryclark/jekyll-now) on GitHub.

```python
import tensorflow as tf
import tf
import tensorflow_hub as hub

embed = hub.Module("https://tfhub.dev/google/"
"universal-sentence-encoder/1")
embedding = embed([
"The quick brown fox jumps over the lazy dog."])


```

Other Stuff
