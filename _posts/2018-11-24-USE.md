---
layout: post
title: Universal Sentence Encoder
---

## What is the Universal Sentence Encoder (USE)?
Google's Universal Sentence Encoder (USE) is a tool that converts a string of words into 512 dimensional vectors. These vectors capture the semantic meaning of the sequence of words in a sentence and therefore can be used as inputs for other downstream NLP tasks like classification, semantic similarity measurement etc. A representative flow for a typical text classification task is shown below:

![_config.yml]({{ site.baseurl }}/images/example-classification (1).png)

USE is made available publicly on TensorFlow Hub. Following is a sample code that shows how to use it:

```python
import tensorflow_hub as hub

embed = hub.Module("https://tfhub.dev/google/"
"universal-sentence-encoder/1")
embedding = embed([
"The quick brown fox jumps over the lazy dog."])
```


## But wait...don't get too excited just yet!

While USE is a fantastic tool and one maybe tempted to use it straight out of the box, it is critical to know the underlying architecture of the model and most importantly, what kind of data it was trained on. For example if you're working with a corpus of scientific articles, it is useful to know that USE is trained on Wikipedia, which has plenty of scientific content. Thus, making the encoder an ideal choice for working with this kind if data. On the other hand, if you're dealing with bank reserach reports or other kinds of business documents, USE might encounter a lot of jargon that it is unfamiliar with which may lead to erroneous results. We shall cover this later in this post.

The Universal Sentence Encoder comes in 2 flavors each with a distinct model architecture:
### 1. The Transformer Architecture 

The Transformer based Universal Sentence Encoder constructs sentence embeddings using the encoder part of the transformer architecture proposed in this [paper](https://arxiv.org/pdf/1706.03762.pdf) by Vaswani et al. On a high level, this architecture uses self-attention to compute context aware representations of words in a sentence that take into account both the **ordering** and **identity** of all the other words. These context aware word vectors are them summed up to output the vector for the entire sentence. A pictorial representation of one encoder is shown below. There are 6 such encoders stacked one on top of the other in the final model. 

![_config.yml]({{ site.baseurl }}/images/Transformer.png)

It is important to note that even though the 'sequentiality' (for lack of better term) of the words is taken into account in this model, we do not use any form of recurrence here. This makes this model highly parallizable. If you would like to delve deeper into this cool architecture and understand its nuts and bolts under the hood, I'd recommend [this awesome blog on the transformer architecture.](http://jalammar.github.io/illustrated-transformer/) 

### 2. The Deep Averaging Network (DAN)

In this architecture the word embeddings for a sentence are first averaged together and then passed through a feed forward neural network as shown below.

![_config.yml]({{ site.baseurl }}/images/DAN.png)

The feed forward neural network is then trained on various downstream tasks like classification, similarity etc. The main advantage of the DAN over the transformer architecture is that the compute time is linear in length of the input sequence. 

The easiest way to make your first post is to edit this one. Go into /_posts/ and update the Hello World markdown file. For more instructions head over to the [Jekyll Now repository](https://github.com/barryclark/jekyll-now) on GitHub.


## The Data on which USE is trained

As I had touched upon earlier, it is important to know what kind of data the encoder was trained on. This will give you an idea of what tasks USE is suitable for. I have tabulated the data and task type here:


 **Task**  |  **Data**  
------------ | -------------
 Unsupervised  |  Wikipedia, Web news, Web question-answer pages and discussion forums.  
Supervised  |  Stanford Natural Language Inference (SNLI) corpus.  
{: .tablelines}


## Classification Example - IMDB movie review sentiment classification

We will now look at USE in action. [Movie reviews from IMDB] (http://ai.stanford.edu/~amaas/data/sentiment/) will be fed into the encoder which will convert them to fixed length 512 dimensional vectors that will then go into a Fully connected Neural Netwrok. Let's dive right into it!

### 1. Setting up the environment:

#### 1.1 Installing required packages
```python
# Install the latest Tensorflow version.
!pip install --quiet "tensorflow>=1.7"
# Install TF-Hub.
!pip install tensorflow-hub
!pip install seaborn
```
#### 1.2 Importing relevant libraries

```python
import tensorflow as tf
import tensorflow_hub as hub
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import re
import seaborn as sns
from tensorflow import keras
import tensorflow as tf
import tensorflow_hub as hub
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import re
import seaborn as sns
from sklearn.model_selection import train_test_split

import keras.layers as layers
from keras.models import Model
from keras import backend as K
np.random.seed(10)
```

### 2. Downloading data and storing it as a DataFrame

```python
# Load all files from a directory in a DataFrame.
def load_directory_data(directory):
  data = {}
  data["sentence"] = []
  data["sentiment"] = []
  for file_path in os.listdir(directory):
    with tf.gfile.GFile(os.path.join(directory, file_path), "r") as f:
      data["sentence"].append(f.read())
      data["sentiment"].append(re.match("\d+_(\d+)\.txt", file_path).group(1))
  return pd.DataFrame.from_dict(data)

# Merge positive and negative examples, add a polarity column and shuffle.
def load_dataset(directory):
  pos_df = load_directory_data(os.path.join(directory, "pos"))
  neg_df = load_directory_data(os.path.join(directory, "neg"))
  pos_df["polarity"] = 1
  neg_df["polarity"] = 0
  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)

# Download and process the dataset files.
def download_and_load_datasets(force_download=False):
  dataset = tf.keras.utils.get_file(
      fname="aclImdb.tar.gz", 
      origin="http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz", 
      extract=True)
  
  train_df = load_dataset(os.path.join(os.path.dirname(dataset), 
                                       "aclImdb", "train"))
  test_df = load_dataset(os.path.join(os.path.dirname(dataset), 
                                      "aclImdb", "test"))
  
  return train_df, test_df

# Reduce logging output.
tf.logging.set_verbosity(tf.logging.ERROR)

train_df, test_df = download_and_load_datasets()
train_df.head()
```

Here's what the training dataframe looks like:

```python
train_df.head()
```

![_config.yml]({{ site.baseurl }}/images/IMDB_train.png)

### 3. Downloading USE 
We will load the encoder module from its URL. 2 corresponds to the DAN architecture while -large/3 corresponds to the Transformer.

https://tfhub.dev/google/universal-sentence-encoder-large/3 - corresponds to the Transformer Architecture
https://tfhub.dev/google/universal-sentence-encoder/2 - corresponds to the DAN architecture

The Transformer one will be larger in size and will take longer to compute embeddings but will also provide more accurate results. Choose the module as per your requirement and willingness to compromise between computation speed and accuracy.
 
```python

module_url = "https://tfhub.dev/google/universal-sentence-encoder-large/3" #@param ["https://tfhub.dev/google/universal-sentence-encoder/2", "https://tfhub.dev/google/universal-sentence-encoder-large/3"]

embed = hub.Module(module_url)

```
### 4. Setting up the Dense Neural Network for classification

Next, we setup the a neural network as shown below. Feel free to play around with the layers and optimizer functions and other hyperparameters.

```python

from keras.optimizers import Adam

optimizer=Adam(lr=0.001, decay=1e-4)
embed_size = 512

input_text = layers.Input(shape=(1,), dtype=tf.string)
embedding = layers.Lambda(UniversalEmbedding, output_shape=(embed_size,))(input_text)
dense1 = layers.Dense(128, activation='relu')(embedding)
dropout1 = layers.Dropout(0.1)(dense1)
#dense2 = layers.Dense(128, activation='relu')(dropout1)
#dropout2 = layers.Dropout(0.1)(dense2)
pred = layers.Dense(2, activation='softmax')(dropout1)
model = Model(inputs=[input_text], outputs=pred)
model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
model.summary()

```

The model can be summarized as follows:

![_config.yml]({{ site.baseurl }}/images/Model_summary.png)


Now that we have the train and test dataframes, we proceed to one hot encode the labels and reshape the data. Also, we will split the training data into training and validation split. The validation data is used as the evaluation set at each step of the training. This ensures that the test data that we have is completely unseen. 

```python
train_text = train_df['sentence'].tolist()
train_text = np.array(train_text, dtype=object)[:, np.newaxis]
train_label = np.asarray(pd.get_dummies(train_df['polarity']), dtype = np.int8)

test_text = test_df['sentence'].tolist()
test_text = np.array(test_text, dtype=object)[:, np.newaxis]
test_label = np.asarray(pd.get_dummies(test_df['polarity']), dtype = np.int8)


val_text, X_test, val_label, y_test = train_test_split(test_text, test_label, stratify = test_label, random_state = 42, test_size = 0.3)
```

### 5. Training the model

```python

with tf.Session() as session:
  K.set_session(session)
  session.run(tf.global_variables_initializer())
  session.run(tf.tables_initializer())
  history = model.fit(train_text, 
            train_label,
            validation_data=(val_text, val_label),
            epochs=10,
            batch_size=250)
  model.save_weights('./imdb_dnn.h5')
  
  ```
  
  Now that we can see that the model has learnt on the training data and we can use the unseen test data to evaluate the true performance of this model. This will give us an idea on how this model will generalize to new unseen data.
  
### 6. Evaluating the model on unseen Test data:
```python

with tf.Session() as session:
  K.set_session(session)
  session.run(tf.global_variables_initializer())
  session.run(tf.tables_initializer())
  model.load_weights('./imdb_dnn.h5')  
  ans = model.evaluate(X_test, y_test)
  
  
print(ans)
```
![_config.yml]({{ site.baseurl }}/images/final_performance.png)

## Conclusion

So we just used the Universal Sentence Encoder as input to a downstream text classification task. We can extend this principle to a variety of other NLP tasks that require a sentence embedding. Hope this also gave you a sneak peak into the the architecture's basics so that the encoder is not a black box to you when you use it. Peace!
























