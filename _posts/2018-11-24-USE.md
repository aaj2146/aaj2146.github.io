---
layout: post
title: Universal Sentence Encoder
---

## What is the Universal Sentence Encoder (USE)?
Google's Universal Sentence Encoder (USE) is a tool that converts a string of words into 512 dimensional vectors. These vectors capture the semantic meaning of the sequence of words in a sentence and therefore can be used as inputs for other downstream NLP tasks like classification, semantic similarity measurement etc. A representative flow for a typical text classification task is shown below:

![_config.yml]({{ site.baseurl }}/images/example-classification (1).png)

USE is made available publicly on TensorFlow Hub. Following is a sample code that shows how to use it:

```python
import tensorflow_hub as hub

embed = hub.Module("https://tfhub.dev/google/"
"universal-sentence-encoder/1")
embedding = embed([
"The quick brown fox jumps over the lazy dog."])
```


## But wait...don't get too excited just yet!

While USE is a fantastic tool and one maybe tempted to use it straight out of the box, it is critical to know the underlying architecture of the model and most importantly, what kind of data it was trained on. For example if you're working with a corpus of scientific articles, it is useful to know that USE is trained on Wikipedia, which has plenty of scientific content. Thus, making the encoder an ideal choice for working with this kind if data. On the other hand, if you're dealing with bank reserach reports or other kinds of business documents, USE might encounter a lot of jargon that it is unfamiliar with which may lead to erroneous results. We shall cover this later in this post.

The Universal Sentence Encoder comes in 2 flavors each with a distinct model architecture:
### 1. The Transformer Architecture 

The Transformer based Universal Sentence Encoder constructs sentence embeddings using the encoder part of the transformer architecture proposed in this [paper](https://arxiv.org/pdf/1706.03762.pdf) by Vaswani et al. On a high level, this architecture uses self-attention to compute context aware representations of words in a sentence that take into account both the **ordering** and **identity** of all the other words. These context aware word vectors are them summed up to output the vector for the entire sentence. A pictorial representation of one encoder is shown below. There are 6 such encoders stacked one on top of the other in the final model. 

![_config.yml]({{ site.baseurl }}/images/Transformer.png)

It is important to note that even though the 'sequentiality' (for lack of better term) of the words is taken into account in this model, we do not use any form of recurrence here. This makes this model highly parallizable. If you would like to delve deeper into this cool architecture and understand its nuts and bolts under the hood, I'd recommend [this awesome blog on the transformer architecture.](http://jalammar.github.io/illustrated-transformer/) 

### 2. The Deep Averaging Network (DAN)

In this architecture the word embeddings for a sentence are first averaged together and then passed through a feed forward neural network as shown below.

![_config.yml]({{ site.baseurl }}/images/DAN.png)

The feed forward neural network is then trained on various downstream tasks like classification, similarity etc. The main advantage of the DAN over the transformer architecture is that the compute time is linear in length of the input sequence. 

The easiest way to make your first post is to edit this one. Go into /_posts/ and update the Hello World markdown file. For more instructions head over to the [Jekyll Now repository](https://github.com/barryclark/jekyll-now) on GitHub.


## The Data on which USE is trained

As I had touched upon earlier, it is important to know what kind of data the encoder was trained on. This will give you an idea of what tasks USE is suitable for. I have tabulated the data and task type here:

Task | Data 
------------ | -------------
Unsupervised | Wikipedia, Web news, Web question-answer pages and discussion forums
Supervised | Stanford Natural Language Inference (SNLI) corpus


<style>
.tablelines table, .tablelines td, .tablelines th {
        border: 1px solid black;
        }
</style>

| P | Q | P * Q |
| - | - | - |
| T | T | T |
| T | F | F |
| F | T | F |
| F | F | F |
{: .tablelines}

|---|---|---|
|a  | b | c|
| 1|2|3|
{: .tablelines}

|-----------------+------------+-----------------+----------------|
| Default aligned |Left aligned| Center aligned  | Right aligned  |
|-----------------|:-----------|:---------------:|---------------:|
| First body part |Second cell | Third cell      | fourth cell    |
| Second line     |foo         | **strong**      | baz            |
| Third line      |quux        | baz             | bar            |
|-----------------+------------+-----------------+----------------|
| Second body     |            |                 |                |
| 2 line          |            |                 |                |
|=================+============+=================+================|
| Footer row      |            |                 |                |
|-----------------+------------+-----------------+----------------|
{: .tablelines}

| A simple | table |
| with multiple | lines|
{: .tablelines}

 **Task**  |  **Data**  
------------ | -------------
 Unsupervised  |  Wikipedia, Web news, Web question-answer pages and discussion forums.  
Supervised  |  Stanford Natural Language Inference (SNLI) corpus.  
{: .tablelines}


